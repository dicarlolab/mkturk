<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Guide - MkTurk</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<h1>Guide</h1>
						<p>Explore what you can achieve with MkTurk</p>
					</header>

					<nav id="nav">
						<ul>
							<li><a href="index.html#intro">Introduction</a></li>
							<li><a href="index.html#capabilities">Core Capabilities</a></li>
							<li><a href="installation.html">Get Started</a></li>
							<li><a href="features.html" class="active">Guide</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Content -->
							<section id="content" class="main">
								
								<!-- <span class="image main"><img src="images/pic04.jpg" alt="" ></span> -->

								<div class="row">
									<div class="col-3 col-12-medium">
										<h3>Table of Contents</h3>
										<ol>
											<li><a href="#scene-composition">3D Scene Composition</a></li>
											<li><a href="#img-filtering">2D Image Filtering</a></li>
											<li><a href="#display">Visual Display</a></li>
											<li><a href="#rendering">Double Buffered Rendering</a></li>
											<li><a href="#cross-device">Consistent Image Display Across Devices</a></li>
											<li><a href="#tasks">Task Specification</a></li>
											<li><a href="#scenes">Scene Specification</a></li>
											<li><a href="#automator">Automator</a></li>
											<li><a href="#cloud-storage">Cloud Data Storage</a></li>
											<li><a href="#cloud-databasing">Cloud Databasing</a></li>
											<li><a href="#realtime-plots">Data Plots in Near Realtime</a></li>
											<li><a href="#client-mirroring">Client Mirroring in Realtime</a></li>
											<li><a href="#web-usb">webUSB Communication</a></li>
											<li><a href="#eye-tracking">Eye Tracking</a></li>
											<li><a href="#hardware">Recommended Hardware</a></li>
											<li><a href="#arduino-shield">Arduino Shield</a></li>
										</ol>
									</div>

									<div class="col-9 col-12-medium">
										<h3><a name="scene-composition" href="https://paper.dropbox.com/doc/MkTurk-Guide--BH_er5LgfK~_f9hLXlZSr5ubAg-Vpip4yINDsGnF3JwvBcJT#:uid=984899673785807537144779&h2=3D-Scene-Composition">3D Scene Composition</a></h3>
										<p style="margin: 0px">
											A scene is composed of two layers:
											<ul style="margin: 0px;">
												<li>Background 2D Image</li>
												<li>Foreground 3D rendered object or scene</li>
											</ul>
											Background 2D images are rendered directly to offscreen canvas while foreground 3D
											content is first rendered on a webGL canvas then transferred to the offscreen 2D canvas.
										</p>
										<span class="image fit" style="border-radius: 0%; border: 0px;">
											<img src="images/guide/opening-movie.gif" alt="" style="border-radius: 0%; "/>
										</span>

										<h3><a name="img-filtering" href="https://paper.dropbox.com/doc/MkTurk-Guide--BH_er5LgfK~_f9hLXlZSr5ubAg-Vpip4yINDsGnF3JwvBcJT#:uid=517364256317495215745593&h2=2D-Image-Filtering">2D Image Filtering</a></h3>
										<p>
											As a post-processing step, 
											<a href="https://developer.mozilla.org/en-US/docs/Web/CSS/filter">image filtering</a>
											can be independently applied to 2D image & 3D scene layers. Included filters:
											<b>brightness, contrast, blur, hue rotate, reverse contrast, grayscale, sepia tone</b>.
										</p>
										<span class="image fit" style="border-radius: 0%; border: 0px;">
											<img src="images/guide/image-filter-mkfiles.png" alt="" style="border-radius: 0%; "/>
										</span>

										<span class="image fit" style="border-radius: 0%; border: 0px;">
											<img src="images/guide/img-filter-examples.png" alt="" style="border-radius: 0%; "/>
										</span>

										<h3><a name="display" href="https://paper.dropbox.com/doc/MkTurk-Guide--BH_er5LgfK~_f9hLXlZSr5ubAg-Vpip4yINDsGnF3JwvBcJT#:uid=257649267602744406466472&h2=Visual-Display">Visual Display</a></h3>
										<p>
											Advances in display hardware (retina display) and software (Javascript Canvas) 
											improved the speed and precision of image display in the browser. Notably:
											<ul>
												<li>Timing: <code>window.requestAnimationFrame()</code> displays canvas content
													locked to the next screen refresh tanks in part to improved timing precision
													from the <code>performance.now()</code> function.
												</li>
												<li>Resolution: retinal displays and in particular those on tablets and
													smartphones are pushing dpi to new levels from >200 on tablets up to 500 dpi
													on the newest Google Pixel phones. This allows greater detail in image content.
													Alternatively, this allows viewers to be positioned much closer to the screen,
													reducing space requirements, while still delivering rich content.
												</li>
											</ul>
										</p>

										<h3><a name="rendering" href="https://paper.dropbox.com/doc/MkTurk-Guide--BH_er5LgfK~_f9hLXlZSr5ubAg-Vpip4yINDsGnF3JwvBcJT#:uid=130186936360383641806722&h2=Double-Buffered-Rendering">Double Buffered Rendering</a></h3>
										<p>
											An invisible, <code>OffscreenCanvas</code> is used for fast double buffered rendering 
											of 2D & 3D content while the current frame is displayed on the visible, 
											onscreen <code>Canvas</code>. When rendering is complete, the fully rendered 
											<code>OffscreenCanvas</code> content is simply committed to the visible onscreen 
											<code>Canvas</code> as a no copy bitmap saving memory.
										</p>

										<h3><a name="cross-device" href="https://paper.dropbox.com/doc/MkTurk-Guide--BH_er5LgfK~_f9hLXlZSr5ubAg-Vpip4yINDsGnF3JwvBcJT#:uid=386003662848886242509638&h2=Consistent-Image-Display-Acros">Consistent Image Display Across Devices</a></h3>
										<p>
											To ensure that an image is displayed at the same size in inches across devices, 
											MkTurk detects the device based on a hardware fingerprint and looks up itâ€™s 
											display pixel density per inches to convert from screen pixels to physical 
											inches (*requires device database).
										</p>
										<span class="image fit" style="border-radius: 0%; border: 0px;">
											<img src="images/guide/cross-device.jpg" alt="" style="border-radius: 0%; "/>
										</span>

										<h3><a name="tasks" href="https://paper.dropbox.com/doc/MkTurk-Guide--BH_er5LgfK~_f9hLXlZSr5ubAg-Vpip4yINDsGnF3JwvBcJT#:uid=891397038932697826271704&h2=Task-Specification">Task Specification</a></h3>
										<p>
											A single JSON file contains the user-controlled task settings (~50 parameters).
											Here the user specifies which scenes to render and whether the subject should
											do a <b>fixation, stimulus-response, match-to-sample, or same-different task</b>.
										</p>
										<span class="image fit" style="border-radius: 0%; border: 0px;">
											<img src="images/guide/agent-params.png" alt="" style="border-radius: 0%; "/>
										</span>

										<h3><a name="scenes" href="https://paper.dropbox.com/doc/MkTurk-Guide--BH_er5LgfK~_f9hLXlZSr5ubAg-Vpip4yINDsGnF3JwvBcJT#:uid=117667244461406404344004&h2=Scene-Specification">Scene Specification</a></h3>
										<p>
											The task parameter file points to the paths for the scene rendering parameter files.
											The scene params file is composed of <b>camera, lights, objects, object filters,
											images and image filters</b> as a JSON object. An integrated smart JSON editor in
											<code>MkFiles</code> allows the user to add various elements (eg, lights, objects)
											starting from stored templates. All parameters such as lighting direction, object
											position and filter strength can be animated by entering an array for a movie sequence
											instead of a fixed scalar for each stimulus.
										</p>
										<span class="image fit" style="border-radius: 0%; border: 0px;">
											<img src="images/guide/scene-specs.png" alt="" style="border-radius: 0%; "/>
										</span>

										<h3><a name="automator" href="https://paper.dropbox.com/doc/MkTurk-Guide--BH_er5LgfK~_f9hLXlZSr5ubAg-Vpip4yINDsGnF3JwvBcJT#:uid=854001258045081235801778&h2=Automator">Automator</a></h3>
										<p>
											A training curriculum can be made by simply concatenating the parameter files
											of each task stage into a single JSON file. The automator will automatically
											move to the next task stage when the specified performance criteria are met
											(eg, 75% accuracy for a period of at least 1000 trials).
										</p>
										<span class="image fit" style="border-radius: 0%; border: 0px;">
											<img src="images/guide/automator.png" alt="" style="border-radius: 0%; "/>
										</span>

										<h3><a name="cloud-storage" href="https://paper.dropbox.com/doc/MkTurk-Guide--BH_er5LgfK~_f9hLXlZSr5ubAg-Vpip4yINDsGnF3JwvBcJT#:uid=490140904917111714643137&h2=Cloud-Data-Storage">Cloud Data Storage</a></h3>
										<p>
											Data generated by a task in <code>MkTurk</code> are stored in a single human
											readable JSON file in cloud storage. All scene rendering parameters are stored
											along with task parameters and the task event data. A temporal hierarchy informs
											where various event streams are piped -- < 10Hz: data sampled at timescales of
											seconds such as trial events (user choices and their timing) are stored in
											the cloud JSON file as well as Cloud Firestore; >10Hz: high frequency timeseries
											such as eye traces are piped to BigQuery tables.
										</p>

										<h3><a name="cloud-databasing" href="https://paper.dropbox.com/doc/MkTurk-Guide--BH_er5LgfK~_f9hLXlZSr5ubAg-Vpip4yINDsGnF3JwvBcJT#:uid=670222053543003455562535&h2=Cloud-Databasing">Cloud Databasing</a></h3>
										<p>
											Databasing supplements Cloud Storage for ease of querying whether in Cloud
											Firestore, a NoSQL JSON database, or BigQuery, a SQL-esque table database.
										</p>

										<h3><a name="realtime-plots" href="https://paper.dropbox.com/doc/MkTurk-Guide--BH_er5LgfK~_f9hLXlZSr5ubAg-Vpip4yINDsGnF3JwvBcJT#:uid=820400910427549158724844&h2=Data-Plots-in-Near-Realtime">Data Plots in Near Realtime</a></h3>
										<p>
											Performance data can be visualized using either Fireplace or Liveplot
											plotting interfaces; plots receive live updates every few seconds.
										  Fireplace plots trial-averaged performance measures
											(accuracy, # of trials) across all agents and sessions by querying the
											Cloud Firestore database. For in-depth analysis of a single agent in a
											session, Liveplot loads the raw JSON from Cloud Storage, plotting per-trial
											responses (touch locations, times).
										</p>
										<span class="image fit" style="border-radius: 0%; border: 0px;">
											<img src="images/guide/liveplot-touch.gif" alt="" style="border-radius: 0%; "/>
										</span>

										<h3><a name="client-mirroring" href="https://paper.dropbox.com/doc/MkTurk-Guide--BH_er5LgfK~_f9hLXlZSr5ubAg-Vpip4yINDsGnF3JwvBcJT#:uid=437560892174924509244968&h2=Client-Mirroring-in-Realtime">Client Mirroring in Realtime</a></h3>
										<p>
											The subject's realtime behavior (mouse, touchpad or eye movements) can be mirrored
											in another browser window. This is done by transmitting screen coordinates via the
											realtime database, allowing for remote monitoring of the behavioral sessions of all
											subjects directly in liveplot. Latency between the mirror and the actual behavior is
											~10 milliseconds.
										</p>
										<span class="image fit" style="border-radius: 0%; border: 0px;">
											<img src="images/guide/realtime-mirror.gif" alt="" style="border-radius: 0%; "/>
										</span>

										<h3><a name="web-usb" href="https://paper.dropbox.com/doc/MkTurk-Guide--BH_er5LgfK~_f9hLXlZSr5ubAg-Vpip4yINDsGnF3JwvBcJT#:uid=139431036608267029405999&h2=webUSB-Communication">webUSB Communication</a></h3>
										<p>
											Chrome's <a href="https://bit.ly/3jD1o9k">webUSB API</a> makes it possible
											to establish fast, secure connections with external devices directly from
											a client-side web browser. <code>MkTurk</code> currently leverages webUSB
											for two-way serial communication with 
											<a href="https://github.com/webusb/arduino">Arduino</a> microcontrollers.
											The Arduino's digital and analog I/Os are interfaced with sensors (photodiode)
											and devices (eye tracker).
										</p>

										<h3><a name="eye-tracking" href="https://paper.dropbox.com/doc/MkTurk-Guide--BH_er5LgfK~_f9hLXlZSr5ubAg-Vpip4yINDsGnF3JwvBcJT#:uid=461304242891918136125233&h2=Eye-Tracking">Eye Tracking</a></h3>
										<p>
											Data from an eye tracker are piped over serial to the Arduino and then
											from the Arduino to the browser using serial over webUSB. Currently, pupil
											position, size, and aspect ratio can be transmitted at 90Hz. 
										</p>
										<span class="image fit" style="border-radius: 0%; border: 0px;">
											<img src="images/guide/eye-tracking-sys-diagram.png" alt="" style="border-radius: 0%; "/>
										</span>


										<h3><a name="hardware" href="https://paper.dropbox.com/doc/MkTurk-Guide--BH_er5LgfK~_f9hLXlZSr5ubAg-Vpip4yINDsGnF3JwvBcJT#:uid=166275801998265099037665&h2=Recommended-Hardware">Recommended Hardware</a></h3>
										<p>
											<ul>
												<li>Touchscreen - <a>Google Pixel C</a> or <a>Google Pixel 4 XL</a></li>
												<li>Pump - <a>Takasago SDMP306D</a> or <a>TCS Micropumps D250S-L</a></li>
												<li>Microcontroller - <a>Arduino Leonardo</a></li>
												<li>USB Battery Pack - <a>Yoobao 20000 mAh Battery</a></li>
												<li>Bluetooth Scale - <a>Motif Mentor</a></li>
											</ul>
										</p>

										<h3><a name="arduino-shield" href="https://paper.dropbox.com/doc/MkTurk-Guide--BH_er5LgfK~_f9hLXlZSr5ubAg-Vpip4yINDsGnF3JwvBcJT#:uid=799149576590404663978977&h2=Arduino-Shield">Arduino Shield</a></h3>
										<p>
											We've designed a custom shield for the Arduino Leonardo that breaks out
											two software serial lines as well as four (x2) analog input lines. The
											analog inputs can be used to receive photodiode signals for keeping track
											of when frames rendered on the physical screen. The software serial ports
											are useful for receiving eye tracker signals or RFID signals.
										</p>
										<span class="image fit" style="border-radius: 0%; border: 0px;">
											<img src="images/guide/arduino-shield.png" alt="" style="border-radius: 0%; "/>
										</span>

										<h3><a href="installation.html">Get Started</a></h3>
										<p>
											To get MkTurk, visit our <a href="installation.html">Get Started</a> page.
										</p>

										
									</div>
								</div>
								
							</section>

							
					</div>

				<!-- Footer -->
				<footer id="footer">
					<section>
						<h2>Issa Lab @ Columbia</h2>
						<p>
							Our lab works at the interface of biological and computational sciences
							closing the loop between theory and experiment. The lab is part of the
							world-class, interdisciplinary research community in the Zuckerman
							Mind Brain Behavior Institute at Columbia.
						</p>
						<ul class="actions">
							<li><a href="https://issalab.neuroscience.columbia.edu/home" class="button">Learn More</a></li>
						</ul>
					</section>
					<section>
						<h2>Contact Us</h2>
						<dl class="alt">
							<dt>Address</dt>
							<dd>3227 Broadway &bull; New York, NY 10027</dd>
							<dt>Phone</dt>
							<dd>212.853.1164</dd>
							<dt>Email</dt>
							<dd><a href="mailto:elias.issa@columbia.edu">elias.issa@columbia.edu</a></dd>
						</dl>
						<ul class="icons">
							<li><a href="https://github.com/issalab/" class="icon brands fa-github alt"><span class="label">GitHub</span></a></li>
						</ul>
					</section>
					<p class="copyright">&copy; IssaLab. Made: <a href="https://hectorcho.com">Hector Cho</a> Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
				</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>